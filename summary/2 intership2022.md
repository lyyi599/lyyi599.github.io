# 数学概论

## 线性代数

* 线性问题：数学对象之间的关系是以一次形式来表示

* 研究对象：标量（scalar）、向量（vector）、矩阵（matrix）

* 张量：计算当中的基本元素

> 线性变换
>
> 定义：设$V_n，U_m$分别是n维和m维的线性空间，T是一个从$V_n$到$U_m$的映射，如果映射满足两个性质：
>
> （1）加和性：T(a1+a2)=T(a1)+T(a2)
>
> （2）齐次性：$\lambda$T($\alpha$) =$\lambda$T($\alpha$)
>
> 则称T为线性变换。
>
> 线性代数可以看作是讨论空间变换与向量运动的科学，而空间变换与向量运动都是线性变换实现的
>
> 线性变换在深度学习中最直观的应用为通过矩阵乘法对图像或语音数据集进行增强。

### 特殊矩阵

#### 转置矩阵

#### 单位矩阵

任意矩阵与单位矩阵相乘等于原矩阵

#### 逆矩阵

满足$A^{-1}A=I_n$

#### 正交矩阵

满足$AA^{T}=A^{T}A=I_n$，则称A为正交矩阵，且$A^{-1}=A^T$

正交矩阵都是行向量与列向量之间两两相交（向量点积为0）的单位矩阵

n阶正交矩阵可以看作n维空间中任意相互垂直坐标基

向量乘以一个正交矩阵，可以看成对向量只做旋转，而没有伸缩和空间映射

#### 对角矩阵

出了对角线之外的元素都是0的矩阵

#### 对称矩阵



### 行列式

是将一个方阵映射到一个标量的函数，记作det(A)或|A|

#### 行列式的含义

* 行列式等于矩阵特征值的乘积
* 行列式的绝对值可以用来衡量矩阵参与矩阵乘法后空间扩大或缩小了多少



### 矩阵分解

#### 特征值和特征向量

定义：设A是数据k上的n阶**方阵**，如果$K^n$中有非零列向量$\alpha$使得$A\alpha=\lambda\alpha$,则称$\lambda$是A的一个特征值，称$\alpha$是A属于特征值$\lambda$的一个特征向量。

一个矩阵对应着一种线性变换，通过矩阵乘法实现对向量的旋转、压缩和映射。

#### 特征分解

利用特征值分解方阵

设**方阵**A有n个线性无关的特征向量$\alpha_n$，相对应的特征值为$\lambda_n$则A的特征分解为：$$A=Pdiag(\lambda)P^{-1}$$

#### 奇异值分解

应用：图像压缩



## 概率论

### 随机实验

满足以下三个特点的实验：

* 可以在相同的条件下重复进行
* 每次实验的可能结果不止一个，并且能够事先明确实验的所有可能结果
* 进行一次试验之前不知道哪个结果会出现



样本点：一个实验所有可能的结果的集合是样本空间，每个结果是一个样本点

随机事件：随机实验的某些样本点组成的集合

随机变量：本质是一个函数，是从样本空间的子集到实数集的一个映射，分为离散随机变量和连续随机变量。

### 分布率

又称概率质量函数



### 特殊离散分布

#### 伯努利分布

也叫0-1分布，两点分布，随机变量X只可能取0和1两个值，分布律写作：
$$
P(X=k)=p^k(1-p)^{1-k},k=0,1(0{\leq}p{\leq}1)
$$

#### 二项分布

n此伯努利分布满足的分布

若X表示n重伯努利实验中事件A发生的次数，则n此实验中实验A发生k次的概率为：
$$
P(X=k)=C^l_np^k(1-p)^{n-k},n=0,1,...,n
$$
此时，称X服从参数n，p的二项分布，记作X~B(n，p)，其中E(X)=np，Var(x)=np(1-p)

#### 泊松分布

若随机变量所有的可能的取值为0，1，2，...，而取每个值的概率为：
$$
P(X=K)={{\lambda^ke^{-\lambda}}\over{k!}},k=0,1,2,...
$$
则称X服从参数为$\lambda$的泊松分布，记为X~P($\lambda$)，其中E(X)=$\lambda$,D(X)=$\lambda$，参数是单位时间或单位面积内随机事件的平均发生率



### 连续型随机变量

#### 正态分布

记作X~N($\mu$,$\sigma^2$)



### 随机向量

多个变量组成的向量



### 贝叶斯定理

#### 条件概率、贝叶斯公式

X是原因，Y是结果（事件发生）。已知原因求解事件发生的概率通常被叫做条件概率，也叫做后验概率：
$$
P(Y|X)={{P(YX)}\over{P(X)}}
$$
通常需要在已知事件发生的情况下计算P(X|Y)，即是事件发生，分析导致事件发生的原因。此时如果还知道先验概率P(X)，我们可以用贝叶斯公式计算：
$$
P(X|Y)={P(XY)\over{P(Y)}}={P(Y|X)P(X)\over{P(Y)}}
$$
分母可以利用全概率公式分解。



## 最优化问题



# 机器学习

## 机器学习算法

典型的三种任务：

* 分类：计算机程序需要指定输入属于k类中的哪一类。为了完成这个任务，学习算法通常会输出一个函数f：$R^n->(1,2,...,k)$。计算机视觉中的图像分类算法解决的就是一个分类任务
* 回归：计算机程序会对给定输入来预测输出数值。学习算法通常会输出一个函数$R^n->R$，这类任务的例子是预测投保人的索赔金额（用于设置保险费），或者预测证券未来的价格
* 聚类：对大量未知标注的数据集，按照数据的内在相似性，将数据划分为多个类别，类别内的数据相似度较大，类别间的相似性较小。可以用在土拍你检索，用户画像等场景

分类和回归是预测问题的两个主要类型，占到80-90%，分类的输出是离散的类别值，而回归的输出是连续数值



都是上课讲的，不想做笔记了



# 深度学习

## 深度学习法则

 深度学习一般指深度神经网络，深度指神经网络的层数（多层）

深度神经网络分为：输出层、隐藏层、输入层

人工神经网络是指一种旨在模仿人类脑结构机器功能的信息处理系统。

单层感知机无法解决XOR问题，出现了多层感知机，也称为前馈神经网络

## 深度学习训练法则

### 梯度下降

梯度向量的方向，指向函数增长最快的方向，因此负梯度向量，指向函数下降最快的方向。

损失函数反映了感知器目标输出和实际输出之间的误差。最常用的误差函数是均方误差。一般均方误差更多用于回归问题，而交叉熵误差更多用于分类问题。

### 反向传播算法

信号正向传播，误差反向传播

### 激活函数

sigmoid函数$f(x)={1\over1+{e^{-x}}}$

tanh函数$f(x)={{{e^{x}}-{e^{-x}}}\over{{e^{x}}+{e^{-x}}}}$

softsign函数$f(x)={x\over{|x|+1}}$

上述函数会导致梯度消失问题。

softmax函数

### 正则化

减少泛化误差的一种方法。

防止过拟合最有效的方法是增加训练集合，训练集合越大，过拟合概率越小。

处理过拟合问题的一些重要正则化方法：L1正则、L2正则、数据集合扩充、Dropout、提前结束训练等方法

### 优化器

在梯度下降算法中，有各种不同的改进版本。在面向对象的语言实现中，往往把不同的梯度下降算法封装成一个对象，称为优化器。

算法改进的目的，包括但不限于：

* 加快算法收敛速度
* 尽量避过或者冲过局部极值
* 减小手工参数的设置难度，主要是学习率
* 目前最常用的是Adam优化器

### 神经网络类型

#### 卷积神经网络CNN

一种前馈神经网络，他的人工神经元可以响应一部分覆盖范围内的周围单元，对于图像处理有出色表现。它包括卷积层、池化层、全连接层。在模式分类领域，由于该网络避免对图像的复杂前期预处理，可以直接输入原始图像，得到广泛应用。

#### 循环神经网络RNN

通过隐藏层节点周期性的连接，来捕捉序列化数据中动态信息的神经网络，可以对序列化的数据进行分类。

存在梯度消失问题。   

#### 生成对抗算法GAN

这属于一种思想，分为生成器和判别器两部分。无监督学习。















